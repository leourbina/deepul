{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"name":"Lecture2.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ajOtPFSLAciN","colab_type":"text"},"source":["# Autoregressive models\n","\n","## RNNs\n","\n","- Interpret them as graphical models/bayes nets\n","- Add position encoding when using to generate images for better performance\n","\n","\n","## [MADE](https://arxiv.org/pdf/1502.03509)\n","\n","Masked Autoencoder for Distribution Estimation\n","\n","- [How to use multiple masks at once?](https://youtu.be/iyEOk8KCRUw?list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP&t=4177)\n","\n","<img src=\"https://www.evernote.com/l/AgsYAUqEV21Nwp_UlobVq0wPKbG5zGDfUV8B/image.png\" alt=\"drawing\" width=\"500\"/>\n","\n","- How to train? \n","\n","Using max likelihood approach:\n","\n","$$\\max_{\\theta}\\sum_{i}\\sum_{k}\\log\\left(P_{\\theta}\\left(x_{k}^{(i)} | x_{1:k-1}^{(i)}\\right)\\right)$$\n","\n","Which is equivalent to maximizing:\n","\n","\n","$$\\max_{\\theta}\\prod_i\\prod_k\\left(P_{\\theta}\\left(x_k^{(i)} | x_{1:k-1}^{(i)}\\right)\\right) = \\max_{\\theta}P_{\\theta}\\left(x_1, x_2, \\dots, x_K\\right)$$\n","\n","- How to sample?\n","\n","At any given moment, you've sampled $x_1, \\dots, x_k$, you can feed those parameters into the network, and evaluate $P(x_{k+1} | x_{1:k})$. \n","\n","No input is required to sample $x_1$, so we can simply run the network and get it. It follows by induction that we can sample $x_k$ for all $k$."]},{"cell_type":"markdown","metadata":{"id":"S0iDBqnSAciO","colab_type":"text"},"source":["## Masked Temporal Convolution\n","\n","<img src=\"https://www.evernote.com/l/Ags00m1nWw9DLaGjOYVjeJKLU6GMx3-DAgsB/image.png\" alt=\"drawing\" width=\"500\"/>\n","\n","- Parameter sharing, aka filters"]},{"cell_type":"markdown","metadata":{"id":"0S8_cEr7AciP","colab_type":"text"},"source":["## Wavenet\n","\n","<img src=\"https://www.evernote.com/l/Agvynh7bkHBD3ZBiESXiF0xu8XQf_laS1jgB/image.png\" alt=\"drawing\" width=\"500\"/>\n","\n","- Interesting activation where both sigmoid and tanh activations are multiplied\n"," - Used for gating what to come through, similar to LSTM\n","- Residual connections\n","\n","- When using for image encoding, use positional encoding"]},{"cell_type":"markdown","metadata":{"id":"RqF7Zri8AciP","colab_type":"text"},"source":["## [PixelCNN](https://arxiv.org/pdf/1606.05328.pdf)\n","\n","\"Wavenet/MADE for 2D\"\n","\n","\n","<img src=\"https://www.evernote.com/l/AgtA7S8jiNZBpa9Kos7GlsxwAukNXzHdkXUB/image.png\" alt=\"drawing\" width=\"200\"/>\n","\n","Sampling is the same, one pixel at a time - it is very slow.\n","\n","Use 3x2 filter instead of 3x3 to avoid blindspot, and the run a 1d filter\n","\n","<img src=\"https://www.evernote.com/l/AgsFgShAnu1O26EoxIK2XZ8V3w8KBge4WOkB/image.png\" alt=\"drawing\" width=\"500\"/>"]},{"cell_type":"markdown","metadata":{"id":"GoEM9HtnAciQ","colab_type":"text"},"source":["## PixelCNN++\n","\n","- Instead of using softmax, we can parameterize the probability of the value of each pixel using a mixture of logistic dists.\n","- Use skip connections for downsampling "]},{"cell_type":"markdown","metadata":{"id":"BH3Wz3HtAciQ","colab_type":"text"},"source":["## Masked Self-Attention\n","\n","**Traditional Attention**\n","\n","q: Query\n","K: keys\n","V: values\n","\n","$$ A(q, K, V) = \\sum_i \\frac{e^{q\\cdot k_i}}{\\sum_j e^{q\\cdot k_j}} v_i $$\n","\n","**Masking**\n","\n","Subtract a large number in the exponents of everything after that pixel in the given ordering\n","\n","$$ A(q, K, V) = \\sum_i \\frac{e^{q\\cdot k_i - masked(k_i, q).10^{10}} }{\\sum_j e^{q\\cdot k_j - masked(k_j, q).10^{10}}} v_i $$\n","\n","More flexible, we can create orderings that are not compatible with convolutions:\n","\n","<img src=\"https://www.evernote.com/l/AgvXTF9ycvRIDIKKzg2ewWtLbGy2t6GF9yQB/image.png\" alt=\"drawing\" width=\"300\"/>"]},{"cell_type":"markdown","metadata":{"id":"WLk1GJx5AciR","colab_type":"text"},"source":["## Class-Conditional PixelCNN\n","\n","Feed in the label of the class as a one-hot encoded as a bias in each of the filters."]},{"cell_type":"markdown","metadata":{"id":"ozQZTDGDAciR","colab_type":"text"},"source":["## Improve Perf\n","\n","* Break the autoregressive constraints by creating a hierarchical pattern"]}]}