{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Lecture3.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"_D1lEd1rlaZV","colab_type":"text"},"source":["# Flow Models\n","\n","Fitting models for when $x$ is a real variable. Softmax is not an option.\n","\n","**Main idea**\n","\n","We can find/pick distribution $Z$, and learn a flow (invertible, differentiable) $f_\\theta(x)$ which maps from the underlying dist of x to $z \\sim Z$. Then this allows us to sample $x$. \n","\n","<img src=\"https://www.evernote.com/l/AgtDIkTXkYpOupUncfMSwTsTf-lqMyWaxHQB/image.png\" alt=\"drawing\" width=\"500\"/>\n","\n","## Density models\n","\n","Def of PDF is function $p$ such that\n","\n","$$P(x \\in [a, b]) = \\int_a^b p(x)dx$$\n","\n","### How to fit density model?\n","\n","Maximum likelihood\n","\n","$$ \\max_{\\theta} \\sum_i \\log p_{\\theta}(x^{(i)}) $$\n","\n","### Example: Gaussian Mixture\n","\n","$$p_{\\theta} = \\sum_{i=1}^k \\pi_i \\mathcal{N}(x; \\mu_i, \\sigma_i^2) $$\n","Where $$\\sum_i \\pi_i = 1$$\n","\n","Then Â \n","\n","$$\\theta = (\\pi_i, \\dots, \\pi_k, \\mu_1, \\dots, \\mu_k, \\sigma_1, \\dots, \\sigma_k)$$\n","\n","<img src=\"https://www.evernote.com/l/AgsZdCVhKFhBEIf6mrBWsH2cpeHmSOYMzKYB/image.png\" alt=\"drawing\" width=\"500\"/>\n","\n","#### Highlights\n","\n","* Does not work well for high dimensional data\n","\n","E.g. when modelling images, only realistic images are generated very close to the mean of the fitted gaussians. Small perturbations of an image results in non-realistic looking images:\n","\n","<img src=\"https://www.evernote.com/l/Agvbod8yTRhGk4fdvQ512A0G_g5ZEjfY52oB/image.png\" alt=\"drawing\" width=\"500\"/>"]},{"cell_type":"markdown","metadata":{"id":"kZXJzoiqlaZW","colab_type":"text"},"source":["## How to fit a general density model?\n","\n","Intuition:\n","\n","<img src=\"https://www.evernote.com/l/Agvz7oLkjAtFWZMnndxjqzDpfFShHl0WDKEB/image.png\" alt=\"drawing\" width=\"500\"/>\n","\n","* How do we ensure that it is normalized?\n","$$\\int_{-\\infty}^{\\infty} p_{\\theta}(x) dx = 1 $$\n","\n","Softmax does not work here.\n","\n","* How to sample?\n","* Latent representation?"]},{"cell_type":"markdown","metadata":{"id":"8PXDZhfylaZX","colab_type":"text"},"source":["## Flows: Main idea\n","\n","<img src=\"https://www.evernote.com/l/AgsOgtDAdhpKn6Hp32wRet3e5yQ5T9LPGOMB/image.png\" alt=\"drawing\" width=\"500\"/>\n","\n","Generally $$z \\sim p_Z(z)$$\n","Normalizing flow $$ z \\sim \\mathcal{N}(0, 1) $$"]},{"cell_type":"markdown","metadata":{"id":"e4khSwF-laZY","colab_type":"text"},"source":["## Train\n","\n","Still use MLE\n","\n","$$\\max_\\theta \\sum_i \\log p_\\theta(x^{(i)})$$\n","\n","### Use change of vars\n","\n","$$z = f_\\theta(x)$$\n","\n","A transformation of a probability density needs to preserve probability mass. i.e.:\n","\n","$$p_\\theta(x)dx = p(z)dz$$\n","\n","Then\n","$$p_\\theta(x) = p(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x}\\right| $$\n","\n","The steeper the slope of $f$ in a region, the lower the density in the image. $f$ needs to be invertible (monotonic) and differentiable.\n"]},{"cell_type":"markdown","metadata":{"id":"faH08Zw1laZY","colab_type":"text"},"source":["### This transforms training into\n","\n","$$\\max_\\theta\\sum_i\\log p_\\theta(x^{(i)}) = \\max_\\theta\\sum_i\\left(\\log p_Z(f_\\theta(x^{(i)})) + \\log\\left|\\frac{\\partial f_\\theta}{\\partial x}(x^{(i)}\\right|\\right) $$\n","\n","Assuming we have an expression of $p_Z$\n","\n","## Flows: Sampling\n","\n","Sample \n","\n","$$z \\sim p_Z(z) \\Rightarrow x = f_\\theta^{-1}(z)$$"]},{"cell_type":"markdown","metadata":{"id":"2EOzanLNlaZZ","colab_type":"text"},"source":["## Example\n","\n","<img src=\"https://www.evernote.com/l/AgsY0_E34QhKZrmtRrFwwlEc_3Nd897mdegB/image.png\" alt=\"drawing\" width=\"500\"/>\n","\n","## How to pick dist Z?\n","\n","Usually uniform or gaussian"]},{"cell_type":"markdown","metadata":{"id":"mqh5Q0a6laZa","colab_type":"text"},"source":["## What to use to learn $f_\\theta$?\n","\n","* Mixture of gaussians, logistics\n","\n","### Neural nets \n","\n","Composition of flows is a flow. So making every layer a flow guarantees that the computed function is a flow:\n","\n","* Sigmoid (works for distributions between 0-1)\n","* Tanh (works for fitting uniform between -1, 1)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IltjB1YilaZa","colab_type":"text"},"source":["## How General?\n","\n","**VERY**\n","\n","Every CDF is a flow from the underlying prob dist to the uniform distribution.\n","\n","If $x \\rightarrow u$ is a flow and and $z\\rightarrow u$ is a flow, then we can invirt this to get a flow $x\\rightarrow u \\rightarrow z$"]},{"cell_type":"markdown","metadata":{"id":"ti80plcqlaZb","colab_type":"text"},"source":["# Autoregressive Flows\n","\n","Sampling:\n","\n","$$ x_1 \\sim p_\\theta(x_1)\\quad x_1 = f_\\theta^{-1}(z_1) $$\n","$$ x_2 \\sim p_\\theta(x_2 | x_1)\\quad x_2 = f_\\theta^{-1}(z_2; x1) $$\n","$$ x_3 \\sim p_\\theta(x_3 | x_1, x_2)\\quad x_3 = f_\\theta^{-1}(z_3; x1, x2) $$\n"]},{"cell_type":"markdown","metadata":{"id":"Oicx5QYzlaZc","colab_type":"text"},"source":["# [AF and IAF](https://youtu.be/JBb5sSC0JoY?list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP&t=4126)\n","\n","<img src=\"https://www.evernote.com/l/AgszxkHnRPxJjqj7shevq50HpzJsIO_RJeUB/image.png\" alt=\"drawing\" width=\"500\"/>\n","\n","Why do we need 1M layers?\n"]},{"cell_type":"markdown","metadata":{"id":"lp1eYyQmlaZd","colab_type":"text"},"source":["# Change many variables at once\n","\n","$$ p(x) = p(z) \\frac{\\text{vol}(dz)}{\\text{vol}(dx)} = p(z)\\left|\\det \\frac{dz}{dx} \\right| $$\n","\n","The determinant's absolute value represents the "]}]}